<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta http-equiv=permissions-policy content="interest-cohort=()">
<meta name=description content="Building a VR simulation titled Neanderthal Tools with Unity XR, while following archaeological research to represent Neanderthal tool making legacy.">
<meta name=author content="Edvinas Danevičius">
<meta property="og:title" content="Neanderthal Tools">
<meta property="og:description" content="Building a VR simulation titled Neanderthal Tools with Unity XR, while following archaeological research to represent Neanderthal tool making legacy.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://edvinas.dev/posts/neanderthal-tools/"><meta property="og:image" content="https://edvinas.dev/posts/neanderthal-tools/simulation.jpg"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-08-27T00:00:00+00:00">
<meta property="article:modified_time" content="2021-08-27T00:00:00+00:00">
<link rel=apple-touch-icon sizes=180x180 href=/icons/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=/icons/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/icons/favicon-16x16.png>
<link rel=manifest href=/icons/site.webmanifest>
<link rel="shortcut icon" href=/icons/favicon.ico>
<meta name=msapplication-TileColor content="#da532c">
<meta name=msapplication-config content="/icons/browserconfig.xml">
<meta name=theme-color content="#ffffff">
<style type=text/css>@font-face{font-family:open sans;font-style:normal;font-weight:400;src:url(/OpenSans-Regular.ttf);font-display:fallback}@font-face{font-family:fira code;font-style:normal;font-weight:400;src:url(/FiraCode-Regular.ttf);font-display:fallback}html{overflow-y:scroll;padding:0;margin:0}body{font-family:open sans,sans-serif;line-height:1.5;background-color:#fffaf7;color:#555;max-width:40em;font-size:16px;padding:1em;margin:0 auto 4em}::selection{color:#fffaf7;background-color:#ff3c3c}header{margin-bottom:1.5em;line-height:1.3}.navigation{display:flex;display:-webkit-flex;justify-content:space-between;align-items:center;flex-flow:row wrap;margin-bottom:4em}.navigation h1{padding-right:1rem}nav a:not(:last-child){margin-right:1em}h1,h2,h3,h4{font-weight:500;color:#333}h1 a{color:#000}h1 a:hover,h2 a:hover{text-decoration:none}header h1,header h2,header h3{margin:0}h1{font-size:2em}h2{margin-bottom:.5em;font-size:1.5em}h3{font-size:1.17em}h4{font-style:italic}a{text-decoration:none;color:#ff3c3c}nav a.active{text-decoration:underline}a:hover{text-decoration:underline}code,pre{background:#eee;overflow-x:auto;font-size:14px;font-family:fira code,monospace}code{word-break:break-all;padding:.1em}pre{padding:1em}.gallery{margin:1em 0}.gallery img{margin-bottom:1em;box-shadow:0 .3rem .7rem rgba(0,0,0,.2);display:block;width:100%}video{box-shadow:0 .3rem .7rem rgba(0,0,0,.2);width:100%}@media only screen and (min-width:480px){.gallery{display:flex;display:-webkit-flex;flex-wrap:wrap;margin:1em -.5em}.gallery img{object-fit:cover;margin:0 .5em 1em;width:calc(100%/3 - 2em);flex:auto}}.articles article{margin-bottom:2em}.article-date{font-size:.9em;color:#888}</style>
<title>
Neanderthal Tools | edvinas.dev
</title>
</head>
<body>
<header class=navigation>
<h1>
<a href=/ title="Home page">edvinas.dev</a>
</h1>
<nav>
<a class=active href=/posts/ title="Posts page">Posts</a>
<a href=/about/ title="About page">About</a>
<a href=/thanks/ title="Thanks page">Thanks</a>
</nav>
</header>
<main>
<article>
<header>
<h2>
Neanderthal Tools
</h2>
<div class=article-date>
Posted <time>2021-08-27</time>
</div>
</header>
<video controls preload=metadata poster=neanderthal-tools.jpg>
<source src=neanderthal-tools.m4v type=video/mp4>
</video>
<h3 id=intro>Intro</h3>
<p>This year I attained my Master&rsquo;s degree in
<a href=https://www.en.aau.dk/education/master/medialogy title="Medialogy programme at Aalborg University" target=_blank rel=noreferrer>Medialogy</a>. For the final project, me and my colleague
<a href=https://www.linkedin.com/in/frederikstief title="Frederik's LinkedIn page" target=_blank rel=noreferrer>Frederik Stief</a> have decided to investigate learning aspects in VR without using text or speech. During the project, we collaborated with
<a href=https://www.cost.eu/cost-action/integrating-neandertal-legacy-from-past-to-present title="COST action - Integrating Neandertal Legacy: From Past to Present" target=_blank rel=noreferrer>COST Action (CA19141)</a> representatives and expert archeologists from
<a href=http://www.unizg.hr/homepage/ title="University of Zagreb homepage" target=_blank rel=noreferrer>University of Zagreb</a> when designing the simulation.</p>
<p>This post will focus only on key technical aspects of the project. If you&rsquo;re just interested in the end result and source code, see
<a href=https://github.com/Edvinas01/neanderthal-tools title="Neanderthal Tools project on GitHub" target=_blank rel=noreferrer>Neanderthal Tools</a> (we couldn&rsquo;t come up with a good name).</p>
<h3 id=unity-tooling-for-vr>Unity tooling for VR</h3>
<p>This time we utilized an official package - Unity XR Interaction Toolkit. So far this is the best solution I have used in Unity for interfacing and working with VR headsets. The components are well-designed, documented and extensible. The best part is that they work really well with the new Input System Unity provides.</p>
<p>However, during development we faced some challenges when deploying for different VR headsets, where we had to utilize the OpenXR Plugin. For the majority of the time, we had no issues with Oculus Rift S or Oculus Quest headsets. Though building for SteamVR (we also targeted HTC Vive) posed a lot of problems. The virtual hands would be offset differently for each headset, input bindings would not work or weird overlay textures would appear.</p>
<p>We resolved some of these problems by installing Beta versions of the Oculus and SteamVR runtimes. Yet to fully fix them, we had to create two different builds for each runtime. As it stands now, the plugin is not yet ready for production when targeting multiple vendors. If you encounter some of these issues,
<a href=https://forum.unity.com/threads/unity-support-for-openxr-in-preview.1023613/ title="Thread on OpenXR preview issues" target=_blank rel=noreferrer>Unity support for OpenXR in preview</a> thread might provide helpful solutions.</p>
<h3 id=loading-multiple-scenes-at-once>Loading multiple scenes at once</h3>
<p>When working with VR (and in general any game) in Unity an essential feature is smoothly transitioning between scenes. Simply loading a scene via <code>SceneManager.LoadScene</code> after fading out the screen introduces a noticeable stutter which in some cases can prompt the VR runtime to display a loading overlay which ruins the gameplay experience. The usual approach for this is to utilise <code>SceneManager.LoadSceneAsync</code> with <code>LoadSceneMode.Additive</code> in addition to always keeping one scene open which acts as an intermediate scene for storing the VR components.</p>
<p>For this project we followed a similar approach to the one used in
<a href=/posts/gneiss/ title="Post on Gneiss - DADIU graduation game">Gneiss</a>, where we always keep one scene open for storing various gameplay systems. This time however, we utilised Multi-Scene editing feature which made the experience of editing two scenes at the same time a lot smoother. The only challenge was to make sure that each time after opening a <code>.scene</code> asset the appropriate scenes would load. To solve this we utilised <code>InitializeOnLoad</code> and <code>OnOpenAsset</code> attributes to hook into Unity&rsquo;s callbacks. You can see this in action in
<a href=https://github.com/Edvinas01/neanderthal-tools/blob/master/Assets/Scripts/Scenes/Editor/SceneBootstrapLoader.cs title="Script for opening multiple scenes at once via an asset" target=_blank rel=noreferrer>SceneBootstrapLoader.cs</a>.</p>
<video controls preload=metadata>
<source src="scenes.m4v#t=0.001" type=video/mp4>
</video>
<p>We also wanted to ensure that after hitting play the scenes would activate in the correct order. To do so, we use <code>EditorSceneManager.RestoreSceneManagerSetup</code> as seen in
<a href=https://github.com/Edvinas01/neanderthal-tools/blob/master/Assets/Scripts/Scenes/Editor/SceneBootstrapLoader.cs title="Script for opening multiple scenes at once via an asset" target=_blank rel=noreferrer>SceneBootstrapLoader.cs</a>. Then, we iterate all loaded scenes one by one and activate them. See <code>Start</code> method in
<a href=https://github.com/Edvinas01/neanderthal-tools/blob/master/Assets/Scripts/Scenes/SceneLoader.cs title="Script for loading multiple scenes in the editor and build" target=_blank rel=noreferrer>SceneLoader.cs</a> for an example.</p>
<p>The multi scene setup has been an essential part of my workflow after using it in
<a href=/posts/gneiss/ title="Post on Gneiss - DADIU graduation game">Gneiss</a> and this project. For future projects I might create a small package to avoid having to set up the boilerplate each time as this is an extremely useful tool which helps to separate concerns.</p>
<h3 id=fading-the-screen-in-and-out>Fading the screen in and out</h3>
<p>To fade the screen in and out when transitioning between scenes there are two approaches that I&rsquo;m aware of: use a post-processing effect to blit the screen; use an overlay UI. Since we were short on time, we took the latter approach as it enables easy customization the fade screen (e.g., you can easily add UI elements such as images).</p>
<p>To achieve this we utilized the Camera Stacking feature, which allows to combine the output of multiple cameras. One camera draws the game view while the other draws the UI, which hosts a white full-screen image used to cover the users view. The configuration process is straight forward, however one thing that was odd is the massive performance hit.</p>
<p>When setting up a Camera Stack, an important option to properly configure is the <code>Culling Mask</code> flag or otherwise the engine renders the scene as many times as there are cameras in the stack. Yet even after correctly specifying this option in our project we were still haunted by a massive performance hit.</p>
<div class=gallery>
<img src=performance-camera.jpg title="Performance using camera only with Culling Mask property" alt="Performance using camera only with Culling Mask property" loading=lazy>
</div>
<p>After playing around with camera settings and looking into the Frame Debugger, we found that creating a new Renderer asset which targets only the UI layer and nothing else solves the issue. We also made sure that this Renderer is used only by the UI camera.</p>
<div class=gallery>
<img src=camera.jpg title="Camera component configured to use custom renderer" alt="Camera component configured to use custom renderer" loading=lazy>
<img src=camera-renderer.jpg title="Renderer asset configured to render only the UI" alt="Renderer asset configured to render only the UI" loading=lazy>
</div>
<div class=gallery>
<img src=performance-renderer.jpg title="Performance using a custom renderer" alt="Performance using a custom renderer" loading=lazy>
</div>
<p>We only spotted this at the end of development as we always thought that the performance penalty was due to running the project from within the editor. Only after taking a look into the Frame Debugger we were able to see that the second camera was doing more work than it&rsquo;s supposed to. Talk about an expensive fade screen.</p>
<h3 id=designing-realistic-audio>Designing realistic audio</h3>
<p>Each interaction in the project has audio feedback. We decided that each object should sound as realistic as possible. For this we went out and recorded actual audio samples of flint, wood and stone. Special thanks to
<a href=https://sutkusaudio.com title="Rokas personal website" target=_blank rel=noreferrer>Rokas Sutkus</a> for helping with audio recording and post-processing! You can also find the recorded samples
<a href=https://github.com/Edvinas01/neanderthal-tools/tree/master/Assets/Audio title="Audio samples recorded for the project" target=_blank rel=noreferrer>here</a> and use them in your projects.</p>
<div class=gallery>
<img src=recording-setup.jpg title="Recording setup" alt="Recording setup" loading=lazy>
<img src=recording-flakes.jpg title="Flint pieces used to record stone sounds" alt="Flint pieces used to record stone sounds" loading=lazy>
</div>
<p>Having realistic audio samples greatly increased the immersion and enjoyment, both of us were quite amazed by how the weight of each action changes. Still, just by using the samples without any runtime modifications sounded plain. To solve this and increase realism even more when executing an action, we group the samples into pools, where each call to a pool provides a random sample. After picking a sample we adjust the attenuation and pitch based on the applied force during the action. This is a rather simple process, however the results are juicy. You can check out the scripts used to achieve this
<a href=https://github.com/Edvinas01/neanderthal-tools/tree/master/Assets/Scripts/Audio title="Audio scripts" target=_blank rel=noreferrer>here</a>.</p>
<video controls preload=metadata>
<source src="audio.m4v#t=0.001" type=video/mp4>
</video>
<h3 id=moving-virtual-hands>Moving virtual hands</h3>
<p>Most VR games I&rsquo;ve tried usually move the virtual hands by ignoring the environment. That is, the hands pass through walls and objects, except when executing actions (e.g., grabbing an object). While this approach provides great accuracy, it has quite a few downsides, the major ones being: the ability to push grabbed objects through walls; hands don&rsquo;t push physics objects.</p>
<p>To workaround this and increase realism, we modeled the hands as physical objects which are moved by using physics. We followed a great tutorial by
<a href="https://www.youtube.com/watch?v=6lK8QXL4bxc" title="VR with Andrew tutorial on how to implement physical hand movement" target=_blank rel=noreferrer>VR with Andrew</a>, where they move the hands by applying linear and angular velocities in order to drive them towards the target destination (orientation of the controllers). You can also checkout our modified
<a href=https://github.com/Edvinas01/neanderthal-tools/blob/master/Assets/Scripts/Hands/PhysicsHand.cs title="Script used for phyiscally moving the virtual hands" target=_blank rel=noreferrer>PhysicsHand.cs</a> script as well.</p>
<p>However, this approach has its own flaws. For example, when holding an object if the user were to push the object into a wall, it would jump around uncontrollably. We&rsquo;ve tried tinkering with applying a varying amount of force based on the resisting collision force, however that introduced oscillations. Additionally, we tried messing around with various Joint setups for grabbing objects, however the results were not satisfying as we couldn&rsquo;t find the right values to avoid the aforementioned issues. Though compared to my previous prototypes, this is a major step in the right direction.</p>
<video controls preload=metadata>
<source src="hands.m4v#t=0.001" type=video/mp4>
</video>
<p>For future projects I&rsquo;ll experiment more with Joints and will try to test different approaches with parenting. The challenge here though is that such tests will most likely require a completely custom grabbing system. At the moment, Unity XR Interaction Toolkit scripts are a bit difficult to work with for such an approach as most of their existing functionality has to be thrown out.</p>
<h3 id=animating-via-vr-motion-capture-software>Animating via VR motion capture software</h3>
<p>The major constraint of the project was to avoid using text or speech. To work around this we decided to utilise animations, particles and audio cues to guide the users. The particle systems and audio cues were rather simple to work with, however for animations we wanted to have a guide character which would show the actions the user has to perform, which introduced quite a lot of complexity.</p>
<video controls preload=metadata>
<source src="animations.m4v#t=0.001" type=video/mp4>
</video>
<p>To quickly implement animations, we looked into motion capture software as we wanted to use our VR headsets and controllers as motion capture devices, and handle the rest with Inverse Kinematics (IK). We landed on experimenting with the Beta version of
<a href=https://store.steampowered.com/app/1540000/Mocap_Fusion__VR/ title="Motion capture software" target=_blank rel=noreferrer>Mocap Fusion [ VR ]</a> which had everything we needed, including IK. We quickly recorded some simple animations and later post-processed them in Blender to make them loopable. The result was surprisingly good - well at least better than anything we could animate by hand.</p>
<video controls preload=metadata>
<source src="animations-mocap.m4v#t=0.001" type=video/mp4>
</video>
<h3 id=closing-notes>Closing notes</h3>
<p>So far this was the most difficult VR project I&rsquo;ve worked on as it involved a lot of tweaking and designing based on research. However, the tooling choice this time around was right as it was always clear on what is happening under the hood of Unity XR. For future projects I&rsquo;ll definitely utilise Unity XR even more and hopefully the Open XR plugin improves as well to make deployment a bit easier.</p>
</article>
</main>
</body>
</html>